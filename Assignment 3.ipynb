{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (37 total marks)\n",
    "### Due: October 24 at 11:59pm\n",
    "\n",
    "### Name: Jubayer Ahmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 1: Regression (14.5 marks)\n",
    "\n",
    "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (0.5 marks)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_concrete\n",
    "X,y = load_concrete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0 marks)\n",
    "\n",
    "Data processing was completed in the previous assignment. No need to repeat here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
    "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
    "3. Implement each machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3f7a8",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fdc93a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Training MSE  Validation MSE\n",
      "DT     47.279761       73.447331\n",
      "RF     29.577455       45.059351\n",
      "GB      3.379440       22.783221\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=5, random_state=0).fit(X_train, y_train)\n",
    "forest = RandomForestRegressor(max_depth=5, random_state=0).fit(X_train, y_train)\n",
    "gradient = GradientBoostingRegressor(max_depth=5, random_state=0).fit(X_train, y_train)\n",
    "models = [tree, forest, gradient]\n",
    "results = pd.DataFrame({\"Training MSE\": [0,0,0], \"Validation MSE\": [0,0,0]}, index=[\"DT\", \"RF\", \"GB\"])\n",
    "\n",
    "for i in range(len(models)):\n",
    "    score = cross_validate(models[i], X_train, y_train, cv=5, \n",
    "                        scoring='neg_mean_squared_error',\n",
    "                       return_train_score=True)\n",
    "    results.loc[results.index[i], 'Training MSE'] = np.mean(score['train_score']) * -1\n",
    "    results.loc[results.index[i], 'Validation MSE'] = np.mean(score['test_score']) * -1\n",
    "\n",
    "print(results)\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31715a9d",
   "metadata": {},
   "source": [
    "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "83539f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Training Score  Validation Score\n",
      "DT        0.834465          0.738697\n",
      "RF        0.896557          0.840927\n",
      "GB        0.988171          0.919471\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "result = pd.DataFrame({\"Training Score\": [0,0,0], \"Validation Score\": [0,0,0]}, index=[\"DT\", \"RF\", \"GB\"])\n",
    "\n",
    "for i in range(len(models)):\n",
    "    score = cross_validate(models[i], X_train, y_train, cv=5, \n",
    "                        scoring='r2',\n",
    "                       return_train_score=True)\n",
    "    result.loc[result.index[i], 'Training Score'] = np.mean(score['train_score']) \n",
    "    result.loc[result.index[i], 'Validation Score'] = np.mean(score['test_score']) \n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
    "1. Out of the models you tested, which model would you select for this dataset and why?\n",
    "1. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. All results from the trees above are better than the linear regression model which only produced ~0.6 for both validation and training. We can also see that the linear model produced higher MSE at 111 for training and 96 for validation. In the linear model, the training and validation MSE and r2 were basically the same while we do see some variance when looking at the tree models. This variance in the tree model is to be expected as we got much higher training score and we would not expect the validation score to increase linearly with the training score. Our goal is to get the peak of the validation curve before it starts to drop off again. Given the low scores in the linear model, we could say it produced high bias since it produced scores on the left side of the traning/variance curves.\n",
    "2. I would pick the gradient boosting models as it has the highest training and validation scores. The decision tree seems to overfit as there is significant variance. It has the highest validaion MSE at 73 and lowest r2 score at 0.74. The Random Forest model did significantly better than the decision tree and it also produced lower variance. However, we can see that the gradient boosting produced even better results with 0.92 for r2 score and 23 for MSE.\n",
    "3. We could try lowering the max depth. This will reduce the training score but may improve the validation score as the model complexity will decrease and be less fitted to the training data. We could also increase min_samples_split to decrease the number of splits which again reduces complexity but could potentially increase the validaiton score. For the RF model, we could increase the number of trees (n_estimator) and n_features to consider more features for each split. These would lead to higher training score. It is possible that it leads to overfit and validation start to drop if increased too much. In the gradient boosting model, we have very high training score and are observing some variance. It is possible that we would benefity from less complexity to reduce training and increase validation. In addition to tweaking n_estimator, we could decrease the learning rate to reduce the contribution of weak learner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 2: Classification (17.5 marks)\n",
    "\n",
    "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset \n",
    "\n",
    "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
    "\n",
    "Print the size and type of `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2314 <class 'pandas.core.frame.DataFrame'>\n",
      "178 <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import wine dataset\n",
    "import os\n",
    "import requests\n",
    "\n",
    "file_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n",
    "file_name = file_url.split('/')[-1]\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "    print('Downloading from {}'.format(file_url))\n",
    "    r = requests.get(file_url)\n",
    "    with open(file_name,'wb') as output_file:\n",
    "        output_file.write(r.content)\n",
    "    \n",
    "data = pd.read_csv(file_name,                 \n",
    "                   na_values='?', \n",
    "                   names=[ 'class', 'Alcohol', 'Malicacid', 'Ash', 'Alcalinity_of_ash', 'Magnesium',\n",
    "                            'Total_phenols', 'Flavanoids', 'Nonflavanoid_phenols', 'Proanthocyanins', 'Color_intensity',\n",
    "                            'Hue', '0D280_0D315_of_diluted_wines', 'Proline'])\n",
    "\n",
    "y = data['class']\n",
    "X = data.drop(columns=['class'])\n",
    "print(X.size, type(X))\n",
    "print(y.size, type(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af110",
   "metadata": {},
   "source": [
    "Print the first five rows of the dataset to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ea266921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  Total_phenols  \\\n",
      "0    14.23       1.71  2.43               15.6        127           2.80   \n",
      "1    13.20       1.78  2.14               11.2        100           2.65   \n",
      "2    13.16       2.36  2.67               18.6        101           2.80   \n",
      "3    14.37       1.95  2.50               16.8        113           3.85   \n",
      "4    13.24       2.59  2.87               21.0        118           2.80   \n",
      "\n",
      "   Flavanoids  Nonflavanoid_phenols  Proanthocyanins  Color_intensity   Hue  \\\n",
      "0        3.06                  0.28             2.29             5.64  1.04   \n",
      "1        2.76                  0.26             1.28             4.38  1.05   \n",
      "2        3.24                  0.30             2.81             5.68  1.03   \n",
      "3        3.49                  0.24             2.18             7.80  0.86   \n",
      "4        2.69                  0.39             1.82             4.32  1.04   \n",
      "\n",
      "   0D280_0D315_of_diluted_wines  Proline  \n",
      "0                          3.92     1065  \n",
      "1                          3.40     1050  \n",
      "2                          3.17     1185  \n",
      "3                          3.45     1480  \n",
      "4                          2.93      735  \n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fc8fe",
   "metadata": {},
   "source": [
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "97c6e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(data.head().isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070956af",
   "metadata": {},
   "source": [
    "How many samples do we have of each type of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b37a6fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "2    71\n",
      "1    59\n",
      "3    48\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
    "2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870b0d2",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "#### Step 5.1: Compare Models\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Training Score  Validation Score\n",
      "DT         0.994357          0.894302\n",
      "SVC        0.680427          0.676638\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "tree = DecisionTreeClassifier(max_depth=3).fit(X_train, y_train)\n",
    "svm = SVC().fit(X_train, y_train)\n",
    "\n",
    "models = [tree, svm]\n",
    "results = pd.DataFrame({\"Training Score\": [0,0], \"Validation Score\": [0,0]}, index=[\"DT\", \"SVC\"])\n",
    "\n",
    "for i in range(len(models)):\n",
    "    score = cross_validate(models[i], X_train, y_train, cv=5, \n",
    "                        scoring='accuracy',\n",
    "                       return_train_score=True)\n",
    "    results.loc[results.index[i], 'Training Score'] = np.mean(score['train_score']) \n",
    "    results.loc[results.index[i], 'Validation Score'] = np.mean(score['test_score'])\n",
    "\n",
    "print(results)\n",
    "\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e17878",
   "metadata": {},
   "source": [
    "#### Step 5.2: Visualize Classification Errors\n",
    "Which method gave the highest accuracy? Use this method to print the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "44b091a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  2  0]\n",
      " [ 0 20  1]\n",
      " [ 0  0  8]]\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Implement best model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = tree.predict(X_test)\n",
    "mat = confusion_matrix(y_test, y_pred, labels=[1,2,3])\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "09d21b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(170.97222222222223, 0.5, 'true value')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: Print confusion matrix using a heatmap\n",
    "sns.heatmap(mat, xticklabels=[1,2,3],  yticklabels=[1,2,3], square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5ef95947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.88      0.93        16\n",
      "           2       0.91      0.95      0.93        21\n",
      "           3       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.94      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred,\n",
    "                            target_names=['1', '2', '3'],\n",
    "                           zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do the training and validation accuracy change depending on the method used? Explain with values.\n",
    "1. What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
    "1. How many samples were incorrectly classified in step 5.2? \n",
    "1. In this case, is maximizing precision or recall more important? Why?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "1. The training and validation accuracry from the tree (0.99 training, 0.91 validation) were significantly better than the SVC model (0.68 training, 0.68 validation). The SVC model produced similar accuracy for both training and validation whereas there is some variance with the tree model. The training score is very high so it is possible that we are overfitting. We could possibly get better validation by reducing complexity (this would reduce the training score). We could reduce the max_depth or increase the samples required to split. \n",
    "2. The SVC did not work as well with the default parameters. We should increase C (increase regularization) to limit feature importance and we should increase gamma to increase the comlexity of the boundary. It is also possible that the default kernel (RBF) is not the best choice (for example if we have a linearly separable dataset).\n",
    "3. Three as clearly shown by the confusion matrix. Two samples were predicted as class 2 while the true class was class 1 and one sample was classified as class 3 while the true class was class 2.\n",
    "4. It depends on the scenario. Assuming that we want to classify wine properly to sell them (cost impact), we should priority precision since we want to charge the correct price most of the time or else the customer will not be happy and will be returning it. However if the collection of wine includes a particuarly rare expensive wine that we must predict correctly even if it comes at the detriment of incorectly classifying other wines (getting more false positive), then we can maximize recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ff8ae",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "1. I got my source code from the lecture and lab codes provided.\n",
    "2. I did the code in order then did the discussion question in order.\n",
    "3. I only used chatGPT to help me chose precision vs recall. It gave me some examples and I was able to extrapolate to my case. No code modifications were required. I used this prompt: how do i know if i should be maximizing precision or recall\n",
    "4. I did not have any challenges because I reviewed all the material in depth before starting the assignment. I read the lecture notes and watched some youtube videos to clarify some concepts (specifically about SVM). I also reviewed all the jupiter notbooks. This allowed me to complete the assignment with minimal to no challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7358d",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "In the classification report, I can clearly observe the trade off between precision and recall. For example, we can see that precision is 1 and recall is 0.88 for class 1 while it is basically the opposite for class 3 (0.89 precision, 1 recall) and somehwere in between for class 2. As discussed in class, precision and recall have an inverse trend as we simply do a swap of the type of mistakes we make (false positive vs false negative). Since the f1 score combines both , we see that it is relatively the same for all three classes. I also observed that the decision tree tends to overfit in all cases. For example it produced a 0.83 training and 0.74 validation for the load concrete while it got 0.99 training and 0.91 validation on the wine data. As discussed in class, this happens because they develop complex boundaries as it grows. When it reaches too much depth, the tree starts splitting on features that it should not. This is why it is better to use ensemble trees to introduce randomness to creates different trees and average out the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "I like that assingment pushes us to interpret results rather than just trying to code. This is far more valuable as coding can be done by anyone (even using chatGPT's help if needed). The skill is in understanding the reulsts and I am glad the assignment focuses on that.\n",
    "I did find the SVC part challenging as I did not understand it from the lectures. Running the code was easy but I had trouble understanding what the model does until I watched some Youtube videos. We went over it too quickly in the first lecture on the topic. The second lecture on the topic was better but I had already done the assignment by then. I found the SVM chapter very interesting. The concepts of mapping into infinite dimensions to separate data is genius."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21e53b",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (3 marks)\n",
    "\n",
    "Repeat Part 2 and compare the support vector machines model used to `LinearSVC(max_iter=5000)`. Does using `LinearSVC` improve the results? Why or why not?\n",
    "\n",
    "Is `LinearSVC` a good fit for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "30fea72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jubayerahmed/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/jubayerahmed/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/jubayerahmed/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/jubayerahmed/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Training Score  Validation Score\n",
      "DT               0.994357          0.894302\n",
      "SVC              0.680427          0.676638\n",
      "LinearSVC        0.928566          0.909972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jubayerahmed/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/jubayerahmed/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linear_svm = LinearSVC(dual=True, max_iter=5000).fit(X_train, y_train)\n",
    "models.append(linear_svm)\n",
    "score = cross_validate(linear_svm, X_train, y_train, cv=5, \n",
    "                        scoring='accuracy',\n",
    "                       return_train_score=True)\n",
    "results.loc['LinearSVC', 'Training Score'] = np.mean(score['train_score']) \n",
    "results.loc['LinearSVC', 'Validation Score'] = np.mean(score['test_score']) \n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc68a4",
   "metadata": {},
   "source": [
    "*ANSWER HERE*\n",
    "\n",
    "Yes, the linear SVC is a good model for this data as it produced the best validation score with the least variance (0.93 trainting, 0.92 validation). This means that our data is linearly separable and explains why the SVC with RBF kernel (default kernel) was not able to produce similar results with default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
