{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Hyperparameter Tuning (32 total marks)\n",
    "### Due: November 22 at 11:59pm\n",
    "\n",
    "### Name: Jubayer Ahmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models and evaluate the results. More details for each step can be found below.\n",
    "\n",
    "### You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "## Step 1: Data Input (4 marks)\n",
    "\n",
    "Import the dataset you will be using. You can download the dataset onto your computer and read it in using pandas, or download it directly from the website. Answer the questions below about the dataset you selected. \n",
    "\n",
    "To find a dataset, you can use the resources listed in the notes. The dataset can be numerical, categorical, text-based or mixed. If you want help finding a particular dataset related to your interests, please email the instructor.\n",
    "\n",
    "**You cannot use a dataset that was used for a previous assignment or in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(101,) <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Import dataset (1 mark)\n",
    "import os\n",
    "import requests\n",
    "\n",
    "file_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/zoo/zoo.data'\n",
    "file_name = file_url.split('/')[-1]\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "    print('Downloading from {}'.format(file_url))\n",
    "    r = requests.get(file_url)\n",
    "    with open(file_name,'wb') as output_file:\n",
    "        output_file.write(r.content)\n",
    "    \n",
    "data = pd.read_csv(file_name,                 \n",
    "                   na_values='?', \n",
    "                   names=[ 'animal', 'hair', 'feathers', 'eggs', 'milk', 'airborne',\n",
    "                            'aquatic', 'predator', 'toothed', 'backbone', 'breathes',\n",
    "                            'venemous', 'fins', 'legs', 'tail', 'domestic', 'catsize', 'class'])\n",
    "\n",
    "\n",
    "y = data['class']\n",
    "#dropping the target\n",
    "X = data.drop(columns=['class'])\n",
    "#dropping animal since it is just an ID, provides no value.\n",
    "X = X.drop(columns=['animal'])\n",
    "print(X.shape, type(X))\n",
    "print(y.shape, type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     hair  feathers  eggs  milk  airborne  aquatic  predator  toothed  \\\n",
      "0       1         0     0     1         0        0         1        1   \n",
      "1       1         0     0     1         0        0         0        1   \n",
      "2       0         0     1     0         0        1         1        1   \n",
      "3       1         0     0     1         0        0         1        1   \n",
      "4       1         0     0     1         0        0         1        1   \n",
      "..    ...       ...   ...   ...       ...      ...       ...      ...   \n",
      "96      1         0     0     1         0        0         0        1   \n",
      "97      1         0     1     0         1        0         0        0   \n",
      "98      1         0     0     1         0        0         1        1   \n",
      "99      0         0     1     0         0        0         0        0   \n",
      "100     0         1     1     0         1        0         0        0   \n",
      "\n",
      "     backbone  breathes  venemous  fins  legs  tail  domestic  catsize  \n",
      "0           1         1         0     0     4     0         0        1  \n",
      "1           1         1         0     0     4     1         0        1  \n",
      "2           1         0         0     1     0     1         0        0  \n",
      "3           1         1         0     0     4     0         0        1  \n",
      "4           1         1         0     0     4     1         0        1  \n",
      "..        ...       ...       ...   ...   ...   ...       ...      ...  \n",
      "96          1         1         0     0     2     1         0        1  \n",
      "97          0         1         1     0     6     0         0        0  \n",
      "98          1         1         0     0     4     1         0        1  \n",
      "99          0         1         0     0     0     0         0        0  \n",
      "100         1         1         0     0     2     1         0        0  \n",
      "\n",
      "[101 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20316765",
   "metadata": {},
   "source": [
    "### Questions (3 marks)\n",
    "\n",
    "1. (1 mark) What is the source of your dataset?\n",
    "1. (1 mark) Why did you pick this particular dataset?\n",
    "1. (1 mark) Was there anything challenging about finding a dataset that you wanted to use?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. https://archive.ics.uci.edu/dataset/111/zoo\n",
    "2. I picked this data because I like animals.\n",
    "3. No it was easy. I just went to the same website as we got wine data from last assignment and picked a different dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing (5 marks)\n",
    "\n",
    "The next step is to process your data. Implement the following steps as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "afc244d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Clean data (if needed)\n",
    "#checking for nulls and if I will need imputing.\n",
    "print(data.head().isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101 entries, 0 to 100\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   hair      101 non-null    int64\n",
      " 1   feathers  101 non-null    int64\n",
      " 2   eggs      101 non-null    int64\n",
      " 3   milk      101 non-null    int64\n",
      " 4   airborne  101 non-null    int64\n",
      " 5   aquatic   101 non-null    int64\n",
      " 6   predator  101 non-null    int64\n",
      " 7   toothed   101 non-null    int64\n",
      " 8   backbone  101 non-null    int64\n",
      " 9   breathes  101 non-null    int64\n",
      " 10  venemous  101 non-null    int64\n",
      " 11  fins      101 non-null    int64\n",
      " 12  legs      101 non-null    int64\n",
      " 13  tail      101 non-null    int64\n",
      " 14  domestic  101 non-null    int64\n",
      " 15  catsize   101 non-null    int64\n",
      "dtypes: int64(16)\n",
      "memory usage: 12.8 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101 entries, 0 to 100\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   hair      101 non-null    category\n",
      " 1   feathers  101 non-null    category\n",
      " 2   eggs      101 non-null    category\n",
      " 3   milk      101 non-null    category\n",
      " 4   airborne  101 non-null    category\n",
      " 5   aquatic   101 non-null    category\n",
      " 6   predator  101 non-null    category\n",
      " 7   toothed   101 non-null    category\n",
      " 8   backbone  101 non-null    category\n",
      " 9   breathes  101 non-null    category\n",
      " 10  venemous  101 non-null    category\n",
      " 11  fins      101 non-null    category\n",
      " 12  legs      101 non-null    category\n",
      " 13  tail      101 non-null    category\n",
      " 14  domestic  101 non-null    category\n",
      " 15  catsize   101 non-null    category\n",
      "dtypes: category(16)\n",
      "memory usage: 3.7 KB\n"
     ]
    }
   ],
   "source": [
    "#making all features categorical as they are distinct and have meaning\n",
    "X = X.astype({col: 'category' for col in ['hair', 'feathers', 'eggs', 'milk', 'airborne',\n",
    "                            'aquatic', 'predator', 'toothed', 'backbone', 'breathes',\n",
    "                            'venemous', 'fins', 'legs', 'tail', 'domestic', 'catsize']})\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "70a8c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement preprocessing steps. Remember to use ColumnTransformer if more than one preprocessing method is needed\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c46b7",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "\n",
    "1. (1 mark) Were there any missing/null values in your dataset? If yes, how did you replace them and why? If no, describe how you would've replaced them and why.\n",
    "2. (1 mark) What type of data do you have? What preprocessing methods would you have to apply based on your data types?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. There were no missing values. If there were, I would delete the row because there would be no way for me to find the missing info. There is only one entry per animal so I cannot fill with most frequent as it would likely be incorrect. I cannot fill with zero either since all the features are applicable. I could alternatively consider dropping the column if the missing values are all in one column and that column has similar values for all. This would indicate that this feature is not that important for classification.\n",
    "2. I only have categorical data as each feature has a distinct value. I used one-hot encoding as there is no order or hierarchy between the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "## Step 3: Implement Machine Learning Model (11 marks)\n",
    "\n",
    "In this section, you will implement three different supervised learning models (one linear and two non-linear) of your choice. You will use a pipeline to help you decide which model and hyperparameters work best. It is up to you to select what models to use and what hyperparameters to test. You can use the class examples for guidance. You must print out the best model parameters and results after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5558a776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:\n",
      "{'classifier': LogisticRegression(max_iter=5000, random_state=0), 'classifier__C': 1.0, 'classifier__fit_intercept': True, 'preprocessor': OneHotEncoder(handle_unknown='ignore')}\n",
      "Best cross-validation train score: 1.00\n",
      "Best cross-validation test score: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Implement pipeline and grid search here. Can add more code blocks if necessary\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pipe = Pipeline(steps=[('preprocessor', encoder),\n",
    "                      ('classifier', LogisticRegression(max_iter=5000, random_state=0))])\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier': [LogisticRegression(max_iter=5000, random_state=0)], \n",
    "     'preprocessor': [encoder],\n",
    "     'classifier__C': [0.1, 1.0, 2.0, 4.0],\n",
    "     'classifier__fit_intercept': [True, False]}]\n",
    "\n",
    "grid_linear = GridSearchCV(pipe, param_grid, cv=3, return_train_score=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                            test_size=0.3, stratify=y,random_state=0)\n",
    "\n",
    "grid_linear.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\\n{}\".format(grid_linear.best_params_))\n",
    "print(\"Best cross-validation train score: {:.2f}\".format(grid_linear.cv_results_['mean_train_score'][grid_linear.best_index_]))\n",
    "print(\"Best cross-validation test score: {:.2f}\".format(grid_linear.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:\n",
      "{'classifier': RandomForestClassifier(max_depth=7, max_features=10, n_estimators=20,\n",
      "                       random_state=0), 'classifier__max_depth': 7, 'classifier__max_features': 10, 'classifier__n_estimators': 20, 'preprocessor': OneHotEncoder(handle_unknown='ignore')}\n",
      "\n",
      "Best cross-validation train score: 1.00\n",
      "Best cross-validation test score: 0.97\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('preprocessor', encoder),\n",
    "                      ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier': [RandomForestClassifier(random_state=0)],\n",
    "     'preprocessor': [encoder], \n",
    "        'classifier__n_estimators': [10, 20, 30],\n",
    "        'classifier__max_depth': [5, 7, 9],\n",
    "        'classifier__max_features': [1, 10, 16]}]\n",
    "\n",
    "gridRF = GridSearchCV(pipe, param_grid, cv=3, return_train_score=True)\n",
    "gridRF.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\\n{}\\n\".format(gridRF.best_params_))\n",
    "print(\"Best cross-validation train score: {:.2f}\".format(gridRF.cv_results_['mean_train_score'][gridRF.best_index_]))\n",
    "print(\"Best cross-validation test score: {:.2f}\".format(gridRF.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:\n",
      "{'classifier__C': 1, 'classifier__gamma': 0.1}\n",
      "\n",
      "Best cross-validation train score: 0.99\n",
      "Best cross-validation test score: 0.94\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('preprocessor', encoder),\n",
    "                      ('classifier', SVC(kernel='rbf', random_state=0))])\n",
    "\n",
    "param_grid = { 'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "gridSVC = GridSearchCV(pipe, param_grid, cv=3, return_train_score=True)\n",
    "gridSVC.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\\n{}\\n\".format(gridSVC.best_params_))\n",
    "print(\"Best cross-validation train score: {:.2f}\".format(gridSVC.cv_results_['mean_train_score'][gridSVC.best_index_]))\n",
    "print(\"Best cross-validation test score: {:.2f}\".format(gridSVC.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7075",
   "metadata": {},
   "source": [
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Do you need regression or classification models for your dataset?\n",
    "1. (2 marks) Which models did you select for testing and why?\n",
    "1. (2 marks) Which model worked the best? Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. I need classification because I am dealing with distinct values. Each entry will belong to groups 1,2,3,4,5,6, or 7.\n",
    "2. I chose to go with the Random Forest model since it gave me the best validation score at 0.97.\n",
    "3. They all worked well as the scores are above 0.9 for all. However the Random forest worked slightly better. Given that my dataset has mostly boolean values, Random Forest might capture non-linear relationships effectively while Logistic Regression assumes a linear relationship between the features and the target. Also my dataset is quite small so it is possible that outliers carried a lot of weight. Both logistic regression and SVC would be more affected by this than Random Forest as discussed in class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "## Step 4: Validate Model (6 marks)\n",
    "\n",
    "Use the testing set to calculate the testing accuracy for the best model determined in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "69e64c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForest\n",
      "Test-set score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Calculate testing accuracy (1 mark)\n",
    "print(\"\\nRandomForest\\nTest-set score: {:.2f}\".format(gridRF.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4529ba",
   "metadata": {},
   "source": [
    "\n",
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Which accuracy metric did you choose? \n",
    "1. (1 mark) How do these results compare to those in part 3? Did this model generalize well?\n",
    "1. (3 marks) Based on your results and the context of your dataset, did the best model perform \"well enough\" to be used out in the real-world? Why or why not? Do you have any suggestions for how you could improve this analysis?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. I used the default score method which computes the accuracy for classification.\n",
    "2. Yes the model generalized well as it got the same test score of 0.97 as we got for validation in part 3.\n",
    "3. Although the model did very good with testing score of 0.97, I would like to have more data to train/test before using in real world. This dataset was only 100 entries with single entry for each animal. We know that animals have lots of variation so we should use more data to train the model on those variations. If we only look at the score of 0.97, we would be tempted to start using it in real world, but we should do more rigurous training/testing. I suspect that with more data that includes variation we would not get training of 1 and test of nearly 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "## Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "\n",
    "\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. I wrote it myself after having reviewed the Imputation Examples 1 and 2 from class. I copied the print statements from the notes to print out the best parameters and results.\n",
    "2. I completed in order 1 step at a time.\n",
    "3. I did not use AI.\n",
    "4. I did not encounter challenges as it is pretty straightforward. Reading through the examples as mentioned above helped me understand what I needed to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "I liked the assignment as it brings eveything we learned thus far together. I found it motivating to see how everything ties in together. I also see that I have much more to learn which seems challenging right now. It feels like it will take a lot more practice and studying to be able to work professionaly in this field. I am happy with the progress thus far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
