{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Hyperparameter Tuning (32 total marks)\n",
    "### Due: November 22 at 11:59pm\n",
    "\n",
    "### Name: Jubayer Ahmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models and evaluate the results. More details for each step can be found below.\n",
    "\n",
    "### You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "## Step 1: Data Input (4 marks)\n",
    "\n",
    "Import the dataset you will be using. You can download the dataset onto your computer and read it in using pandas, or download it directly from the website. Answer the questions below about the dataset you selected. \n",
    "\n",
    "To find a dataset, you can use the resources listed in the notes. The dataset can be numerical, categorical, text-based or mixed. If you want help finding a particular dataset related to your interests, please email the instructor.\n",
    "\n",
    "**You cannot use a dataset that was used for a previous assignment or in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 18) <class 'pandas.core.frame.DataFrame'>\n",
      "       animal  hair  feathers  eggs  milk  airborne  aquatic  predator  \\\n",
      "0    aardvark     1         0     0     1         0        0         1   \n",
      "1    antelope     1         0     0     1         0        0         0   \n",
      "2        bass     0         0     1     0         0        1         1   \n",
      "3        bear     1         0     0     1         0        0         1   \n",
      "4        boar     1         0     0     1         0        0         1   \n",
      "..        ...   ...       ...   ...   ...       ...      ...       ...   \n",
      "96    wallaby     1         0     0     1         0        0         0   \n",
      "97       wasp     1         0     1     0         1        0         0   \n",
      "98       wolf     1         0     0     1         0        0         1   \n",
      "99       worm     0         0     1     0         0        0         0   \n",
      "100      wren     0         1     1     0         1        0         0   \n",
      "\n",
      "     toothed  backbone  breathes  venemous  fins  legs  tail  domestic  \\\n",
      "0          1         1         1         0     0     4     0         0   \n",
      "1          1         1         1         0     0     4     1         0   \n",
      "2          1         1         0         0     1     0     1         0   \n",
      "3          1         1         1         0     0     4     0         0   \n",
      "4          1         1         1         0     0     4     1         0   \n",
      "..       ...       ...       ...       ...   ...   ...   ...       ...   \n",
      "96         1         1         1         0     0     2     1         0   \n",
      "97         0         0         1         1     0     6     0         0   \n",
      "98         1         1         1         0     0     4     1         0   \n",
      "99         0         0         1         0     0     0     0         0   \n",
      "100        0         1         1         0     0     2     1         0   \n",
      "\n",
      "     catsize  class  \n",
      "0          1      1  \n",
      "1          1      1  \n",
      "2          0      4  \n",
      "3          1      1  \n",
      "4          1      1  \n",
      "..       ...    ...  \n",
      "96         1      1  \n",
      "97         0      6  \n",
      "98         1      1  \n",
      "99         0      7  \n",
      "100        0      2  \n",
      "\n",
      "[101 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import dataset (1 mark)\n",
    "\n",
    "file_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/zoo/zoo.data'\n",
    "file_name = file_url.split('/')[-1]\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "    print('Downloading from {}'.format(file_url))\n",
    "    r = requests.get(file_url)\n",
    "    with open(file_name,'wb') as output_file:\n",
    "        output_file.write(r.content)\n",
    "    \n",
    "data = pd.read_csv(file_name,                 \n",
    "                   na_values='?', \n",
    "                   names=[ 'animal', 'hair', 'feathers', 'eggs', 'milk', 'airborne',\n",
    "                            'aquatic', 'predator', 'toothed', 'backbone', 'breathes',\n",
    "                            'venemous', 'fins', 'legs', 'tail', 'domestic', 'catsize', 'class'])\n",
    "\n",
    "print(data.shape, type(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20316765",
   "metadata": {},
   "source": [
    "### Questions (3 marks)\n",
    "\n",
    "1. (1 mark) What is the source of your dataset?\n",
    "1. (1 mark) Why did you pick this particular dataset?\n",
    "1. (1 mark) Was there anything challenging about finding a dataset that you wanted to use?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. I got it from the UC Irvine ML libray. Link: https://archive.ics.uci.edu/dataset/111/zoo\n",
    "2. I picked this data because I like animals.\n",
    "3. No it was easy. I just went to the same website as we got wine data from last assignment and picked a different dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing (5 marks)\n",
    "\n",
    "The next step is to process your data. Implement the following steps as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "afc244d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101 entries, 0 to 100\n",
      "Data columns (total 18 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   animal    101 non-null    object\n",
      " 1   hair      101 non-null    int64 \n",
      " 2   feathers  101 non-null    int64 \n",
      " 3   eggs      101 non-null    int64 \n",
      " 4   milk      101 non-null    int64 \n",
      " 5   airborne  101 non-null    int64 \n",
      " 6   aquatic   101 non-null    int64 \n",
      " 7   predator  101 non-null    int64 \n",
      " 8   toothed   101 non-null    int64 \n",
      " 9   backbone  101 non-null    int64 \n",
      " 10  breathes  101 non-null    int64 \n",
      " 11  venemous  101 non-null    int64 \n",
      " 12  fins      101 non-null    int64 \n",
      " 13  legs      101 non-null    int64 \n",
      " 14  tail      101 non-null    int64 \n",
      " 15  domestic  101 non-null    int64 \n",
      " 16  catsize   101 non-null    int64 \n",
      " 17  class     101 non-null    int64 \n",
      "dtypes: int64(17), object(1)\n",
      "memory usage: 14.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Clean data (if needed)\n",
    "#checking for nulls and if I will need imputing.\n",
    "print(data.head().isnull().sum().sum())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "70a8c127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) <class 'pandas.core.frame.DataFrame'>\n",
      "(101,) <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Implement preprocessing steps. Remember to use ColumnTransformer if more than one preprocessing method is needed\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), ['legs'])\n",
    "    ],\n",
    "    remainder='passthrough' \n",
    ")\n",
    "\n",
    "#Dropping animal as it is just an ID column and provides no value for predicting.\n",
    "data = data.drop(columns=['animal'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "y = data['class']\n",
    "X = data.drop(columns=['class'])\n",
    "print(X.shape, type(X))\n",
    "print(y.shape, type(y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c46b7",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "\n",
    "1. (1 mark) Were there any missing/null values in your dataset? If yes, how did you replace them and why? If no, describe how you would've replaced them and why.\n",
    "2. (1 mark) What type of data do you have? What preprocessing methods would you have to apply based on your data types?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. There were no missing values. If there were, I would delete the row because there would be no way for me to find the missing info. There is only one entry per animal so I cannot fill with most frequent as it would likely be incorrect. I cannot fill with zero either since all the features are applicable. I could alternatively consider dropping the column if the missing values are all in one column and that column has similar values for all. This would indicate that this feature is not that important for classification.\n",
    "2. I have a mixture of numerical and categorical values. The animal column is just an ID column where each entry is unique and can be dropped since it does not help in predicting in which group the animal belongs. After dropping this column, I only have numerical values. The legs column represents the leg count of the animal and can be scaled. The rest of columns in the feature matrix are 0 and 1s representing boolean value and are already suited for machine learning. They don't require scaling as they are all in same range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "## Step 3: Implement Machine Learning Model (11 marks)\n",
    "\n",
    "In this section, you will implement three different supervised learning models (one linear and two non-linear) of your choice. You will use a pipeline to help you decide which model and hyperparameters work best. It is up to you to select what models to use and what hyperparameters to test. You can use the class examples for guidance. You must print out the best model parameters and results after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5558a776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'classifier__C': 10, 'classifier__fit_intercept': False}\n",
      "Best cross-validation train score: 1.00\n",
      "Best cross-validation test score: 0.94\n",
      "\n",
      "Training Accuracy Score:  [0.8565106  0.86268344 0.98742138 0.98113208 1.         1.\n",
      " 1.         1.        ]\n",
      "Validation Accuracy Score:  [0.85090218 0.86324786 0.92497626 0.91263058 0.92497626 0.93732194\n",
      " 0.93732194 0.93732194]\n"
     ]
    }
   ],
   "source": [
    "# Implement pipeline and grid search here. Can add more code blocks if necessary\n",
    "\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(max_iter=5000, random_state=1))])\n",
    "\n",
    "param_grid = {\n",
    "     'classifier__C': [0.1, 1, 10, 100],\n",
    "     'classifier__fit_intercept': [True, False]}\n",
    "\n",
    "grid_linear = GridSearchCV(pipe, param_grid=param_grid, cv=3, return_train_score=True)\n",
    "grid_linear.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\\n{}\".format(grid_linear.best_params_))\n",
    "print(\"Best cross-validation train score: {:.2f}\".format(grid_linear.cv_results_['mean_train_score'][grid_linear.best_index_]))\n",
    "print(\"Best cross-validation test score: {:.2f}\".format(grid_linear.best_score_))\n",
    "print(\"\\nTraining Accuracy Score: \", grid_linear.cv_results_['mean_train_score'])\n",
    "print(\"Validation Accuracy Score: \", grid_linear.cv_results_['mean_test_score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'classifier__max_depth': 5, 'classifier__max_features': 10, 'classifier__n_estimators': 9}\n",
      "\n",
      "Best cross-validation train score: 0.99\n",
      "Best cross-validation test score: 0.96\n",
      "\n",
      "Training Accuracy Score:  [0.91893781 0.93745632 0.93745632 0.93745632 0.93139995 0.93757279\n",
      " 0.92522711 0.93139995 0.90041929 0.92499418 0.92511065 0.91905427\n",
      " 0.89412998 0.91276497 0.90018635 0.88784067 0.98742138 0.99371069\n",
      " 0.99371069 1.         0.98742138 0.98742138 0.99371069 0.99371069\n",
      " 0.9750757  0.99371069 0.98136501 0.99371069 0.97495924 0.98742138\n",
      " 0.9750757  0.9750757  0.99371069 1.         1.         1.\n",
      " 0.98742138 0.99371069 0.99371069 0.99371069 0.98742138 0.99371069\n",
      " 0.99371069 0.99371069 0.98113208 0.99371069 0.99371069 0.99371069]\n",
      "Validation Accuracy Score:  [0.88793922 0.88793922 0.88793922 0.88793922 0.8622982  0.89981007\n",
      " 0.89981007 0.88746439 0.84995252 0.86277303 0.87511871 0.87464387\n",
      " 0.8622982  0.87511871 0.87511871 0.8622982  0.95014245 0.93779677\n",
      " 0.93779677 0.93779677 0.9620133  0.9620133  0.96248813 0.96248813\n",
      " 0.9620133  0.94966762 0.93732194 0.95014245 0.94966762 0.93732194\n",
      " 0.93732194 0.93779677 0.9620133  0.93732194 0.93732194 0.95014245\n",
      " 0.9620133  0.95014245 0.96248813 0.96248813 0.9620133  0.95014245\n",
      " 0.95014245 0.95014245 0.94966762 0.94966762 0.94966762 0.96248813]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier(random_state=1))])\n",
    "\n",
    "param_grid = {\n",
    "        'classifier__n_estimators': [5, 7, 9, 11],\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "        'classifier__max_features': [8, 10, 12, 14]}\n",
    "\n",
    "grid_RF = GridSearchCV(pipe, param_grid, cv=3, return_train_score=True)\n",
    "grid_RF.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\\n{}\\n\".format(grid_RF.best_params_))\n",
    "print(\"Best cross-validation train score: {:.2f}\".format(grid_RF.cv_results_['mean_train_score'][grid_RF.best_index_]))\n",
    "print(\"Best cross-validation test score: {:.2f}\".format(grid_RF.best_score_))\n",
    "print(\"\\nTraining Accuracy Score: \", grid_RF.cv_results_['mean_train_score'])\n",
    "print(\"Validation Accuracy Score: \", grid_RF.cv_results_['mean_test_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'classifier__C': 20, 'classifier__gamma': 0.01}\n",
      "\n",
      "Best cross-validation train score: 0.99\n",
      "Best cross-validation test score: 0.94\n",
      "\n",
      "Training Accuracy Score:  [0.41253203 0.60621943 0.92499418 1.         0.61250874 0.9687864\n",
      " 1.         1.         0.79396692 0.98742138 1.         1.\n",
      " 0.825297   0.99371069 1.         1.        ]\n",
      "Validation Accuracy Score:  [0.41263058 0.60018993 0.9002849  0.86324786 0.61253561 0.91263058\n",
      " 0.92497626 0.88793922 0.78822412 0.93779677 0.92497626 0.88793922\n",
      " 0.8005698  0.93779677 0.92497626 0.88793922]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', SVC(kernel='rbf', random_state=1))])\n",
    "\n",
    "param_grid = { 'classifier__C': [1, 10, 20, 30],\n",
    "    'classifier__gamma': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "gridSVC = GridSearchCV(pipe, param_grid, cv=3, return_train_score=True)\n",
    "gridSVC.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\\n{}\\n\".format(gridSVC.best_params_))\n",
    "print(\"Best cross-validation train score: {:.2f}\".format(gridSVC.cv_results_['mean_train_score'][gridSVC.best_index_]))\n",
    "print(\"Best cross-validation test score: {:.2f}\".format(gridSVC.best_score_))\n",
    "print(\"\\nTraining Accuracy Score: \", gridSVC.cv_results_['mean_train_score'])\n",
    "print(\"Validation Accuracy Score: \", gridSVC.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7075",
   "metadata": {},
   "source": [
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Do you need regression or classification models for your dataset?\n",
    "1. (2 marks) Which models did you select for testing and why?\n",
    "1. (2 marks) Which model worked the best? Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. I need classification because I am dealing with distinct values. Each entry will belong to groups 1,2,3,4,5,6, or 7.\n",
    "2. I chose to go with the Random Forest model since it gave me the best validation score at 0.96 which is close to the training score of 0.99. This gave me the best validation score and lowest variance. The logistic regression shows overfit (high variance, low bias) as the training score is 1 while also performing worse than Random Forest in the validation score with a score of 0.94. The SVC gave similar results as the logistic regression with training score of 0.99 and validation of 0.94. I ensured that the selected hyperparameters are in the middle of the selected range. This ensures that I selected a good range for the parameter grid.\n",
    "3. As discussed above, the random forest worked best. Given that my dataset has mostly boolean values, Random Forest might capture non-linear relationships effectively while Logistic Regression assumes a linear relationship between the features and the target. Also my dataset is quite small so it is possible that outliers carried a lot of weight. The SVC and LogisticRegression would be more affected by this than Random Forest as discussed in class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "## Step 4: Validate Model (6 marks)\n",
    "\n",
    "Use the testing set to calculate the testing accuracy for the best model determined in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "69e64c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForest\n",
      "Test-set score: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Calculate testing accuracy (1 mark)\n",
    "print(\"\\nRandomForest\\nTest-set score: {:.2f}\".format(grid_RF.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4529ba",
   "metadata": {},
   "source": [
    "\n",
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Which accuracy metric did you choose? \n",
    "1. (1 mark) How do these results compare to those in part 3? Did this model generalize well?\n",
    "1. (3 marks) Based on your results and the context of your dataset, did the best model perform \"well enough\" to be used out in the real-world? Why or why not? Do you have any suggestions for how you could improve this analysis?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. I used the default score method which computes the accuracy for classification.\n",
    "2. Yes the model generalized well as it got a test score of 0.95 which is close to the validation score in part 3 (0.96). \n",
    "3. Although the model did good with a test score of 0.95, I would like to have more data to train/test before using in real world. This dataset was only 100 entries with single entry for each animal. How we split the data can affect the results with this small dataset as some groups have very small number of samples. This imbalance might affect the model's ability to learn patterns accurately for those small groups. Also, we know that animals have lots of variation so we should use more data to train the model on those variations. This will create a more robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "## Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "\n",
    "\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. I wrote it myself after having reviewed the Imputation Examples 1 and 2 from class. I copied the print statements from these examples to print out the best parameters.\n",
    "2. I completed in order 1 step at a time.\n",
    "3. I only used chatGPT to learn how to specify which columns to scale and which to leave unchanged. I used this querry: \"how to specify on pipeline which columns to scale and which column to keep unchanged\". It told me to use \"remainder = 'passthrough'. \n",
    "4. I did not encounter challenges as it is pretty straightforward. Reading through the examples as mentioned above helped me understand what I needed to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "I liked the assignment as it brings eveything we learned thus far together. I found it motivating to see how everything ties in together. I also see that I have much more to learn which seems challenging right now. It feels like it will take a lot more practice and studying to be able to work professionaly in this field. I am happy with the progress thus far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
