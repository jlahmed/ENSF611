{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: Jubayer Ahmed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262200 <class 'pandas.core.frame.DataFrame'>\n",
      "4600 <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_spam\n",
    "\n",
    "# TO DO: Print size and type of X and y\n",
    "X,y = load_spam()\n",
    "print(X.size, type(X))\n",
    "print(y.size, type(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "print(X.isnull().sum().sum())\n",
    "print(y.isna().sum().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create X_small and y_small \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_small, _, y_small, _ = train_test_split(X, y, train_size = 0.05, stratify=y, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Trained Data Size  Training Score  Validation\n",
      "Using all features             196650        0.933913    0.931304\n",
      "Using two features               6900        0.619420    0.605217\n",
      "Using 5% of data                13110        0.936047    0.965517\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Using X,y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=0)\n",
    "log = LogisticRegression(max_iter=2000).fit(X_train, y_train)\n",
    "full_t = log.score(X_train, y_train)\n",
    "full_v = log.score(X_test, y_test)\n",
    "\n",
    "#Using only first two columns\n",
    "X_t, X_v, y_t, y_v = train_test_split(X.iloc[:,0:2], y, stratify=y,random_state=0)\n",
    "l = LogisticRegression(max_iter=2000).fit(X_t, y_t)\n",
    "two_t = l.score(X_t, y_t)\n",
    "two_v = l.score(X_v, y_v)\n",
    "\n",
    "#Using 5% of data\n",
    "\n",
    "X_small5, X_val, y_small5, y_val = train_test_split(X_small, y_small, stratify=y_small, random_state=0)\n",
    "lg = LogisticRegression(max_iter=2000).fit(X_small, y_small)\n",
    "small_t = lg.score(X_small5, y_small5)\n",
    "small_v = lg.score(X_val, y_val)\n",
    "\n",
    "data = {\"Trained Data Size\": [X_train.size, X_t.size, X_small.size], \"Training Score\": [full_t, two_t, small_t], \"Validation\": [full_v, two_v, small_v]}\n",
    "results = pd.DataFrame(data, index=[\"Using all features\", \"Using two features\", \"Using 5% of data\"])\n",
    "print(results)\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "\n",
    "\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "1. We can see that when we are using only the first two features, we get low training and validation scores indicating that our model is not not performing well. This means that our model would benefit from using more features. We have a low variance since the training and validation scores are close but high bias since they are off the true valiues. This indicates an underfit.\n",
    "\n",
    "When using all features, we can see that the training and validation are high whether we are using 5% of the data for training or the default 75%. One difference is that when using only 5% of the data, we see that the validation score is higher than the training score. This means that the model may be too simple. In this case we have low variance and low bias since the scores are high and close to each other.\n",
    "\n",
    "2. A false positive would be identifying a non-spam email as spam wheras a false negative would be identifying a spam email as non spam. A false negative is worse since if would cause the user to interact with the author of the spam email wheareas a false positive would just cause a non-spam email to go to the junk folder which usually has less negative impact since the author of the email can reach the user using a different approach (such as phone call). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code? \n",
    "2. In what order did you complete the steps?\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not? \n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "1. From the note/lab material\n",
    "2. I did in order they are posted\n",
    "3. I did not use AI\n",
    "4.  I had difficulty writing the code. I was able to overcome it by looking at the code snipets from class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8240 <class 'pandas.core.frame.DataFrame'>\n",
      "1030 <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_concrete\n",
    "\n",
    "# TO DO: Print size and type of X and y\n",
    "X, y = load_concrete()\n",
    "print(X.size, type(X))\n",
    "print(y.size, type(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "print(X.isnull().sum().sum())\n",
    "print(y.isna().sum().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "b5041945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "# Note: for any random state parameters, you can use random_state = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "970c038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_train = model.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_val = r2_score(y_val, y_pred_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Train  Validation\n",
      "MSE  111.358439   95.904136\n",
      "R2     0.610823    0.623414\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "results = pd.DataFrame({'Train': [mse_train, r2_train], 'Validation' : [mse_val, r2_val]}, index=[\"MSE\", \"R2\"])\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
    "    No because the r2 value is a lot lower than 1 for both the training and validation scores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "2. In what order did you complete the steps?In the steps they are posted\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "1. I looked up how to run the mse and r2 commands on chatGPT\n",
    "2. In the steps they are posted\n",
    "3. I typed this into chatGPT: sklearn metric r2 score and mean square error \n",
    "4. No challenges, it was pretty straight forward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "We can see that the R2 score for the training and validation sets are very low (0.61 and 0.62 respectively). We can also see that the mean squarred error is high. These indactors point to a poorly performing model. This means we might need a more complex model. We have low variance between the training and validation sets given that the r2 scores are almost identical. This also points to a high bias since there is a significant difference bwetween the predicted value and true value. Overall the model is not performing well since it underfits the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "I liked that we were asked to discuss our answers which enforced the learning. It made me realize the true value of the class. I realize that it is not about just running comands. This is also motivates me to keep learning now that I understand the value of the class better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for lasso: \n",
      "             Training Score  Validation Score\n",
      "alpha=0.01        0.610823          0.623429\n",
      "alpha=0.1         0.610821          0.623562\n",
      "alpha=1           0.610609          0.624669\n",
      "alpha=10          0.604314          0.626774\n",
      "alpha=100         0.467576          0.507413\n",
      "\n",
      "Results for ridge: \n",
      "             Training Score  Validation Score\n",
      "alpha=0.01        0.610823          0.623429\n",
      "alpha=0.1         0.610821          0.623562\n",
      "alpha=1           0.610609          0.624669\n",
      "alpha=10          0.604314          0.626774\n",
      "alpha=100         0.467576          0.507413\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "lasso001 = Lasso(alpha=0.01).fit(X_train, y_train)\n",
    "lasso01 = Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "lasso1 = Lasso(alpha=1).fit(X_train, y_train)\n",
    "lasso10 = Lasso(alpha=10).fit(X_train, y_train)\n",
    "lasso100 = Lasso(alpha=100).fit(X_train, y_train)\n",
    "\n",
    "ridge001 = Ridge(alpha=0.01).fit(X_train, y_train)\n",
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "ridge1 = Ridge(alpha=1).fit(X_train, y_train)\n",
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "ridge100 = Ridge(alpha=100).fit(X_train, y_train)\n",
    "\n",
    "data = {\"Training Score\": [lasso001.score(X_train, y_train), lasso01.score(X_train, y_train), lasso1.score(X_train, y_train), lasso10.score(X_train, y_train),\n",
    "                           lasso100.score(X_train, y_train)], \"Validation Score\": [lasso001.score(X_val, y_val), lasso01.score(X_val, y_val),lasso1.score(X_val, y_val),\n",
    "                                                                                   lasso10.score(X_val, y_val), lasso100.score(X_val, y_val)]}\n",
    "results = pd.DataFrame(data, index=[\"alpha=0.01\", \"alpha=0.1\", \"alpha=1\", \"alpha=10\",\"alpha=100\"])\n",
    "\n",
    "data1 = {\"Training Score\": [ridge001.score(X_train, y_train), ridge01.score(X_train, y_train), ridge1.score(X_train, y_train), ridge10.score(X_train, y_train),\n",
    "                           ridge100.score(X_train, y_train)], \"Validation Score\": [ridge001.score(X_val, y_val), ridge01.score(X_val, y_val),ridge1.score(X_val, y_val),\n",
    "                                                                                   ridge10.score(X_val, y_val), ridge100.score(X_val, y_val)]}\n",
    "results1 = pd.DataFrame(data, index=[\"alpha=0.01\", \"alpha=0.1\", \"alpha=1\", \"alpha=10\",\"alpha=100\"])\n",
    "\n",
    "print(\"Results for lasso: \\n\", results)\n",
    "print(\"\\nResults for ridge: \\n\", results1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "*ANSWER HERE*\n",
    "\n",
    "We can see that varying alpha (L1 regularization in lasso and L2 in ridge) does not impact the outcome of the training and validation scores as they remain consistently low. This suggests that there is no one feature that is more important in predicting the outcome. Reducing some coefficient (L2) or eliminating some coefficient (L1) does not help us create a good model. This suggests that our data follows a standard linear regression model more closely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
